---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Deep surv}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(ReSurv)

```

```{r}
# Input data

input_data <- data_generator(random_seed = 1964)


```


```{r}
# # # ## Model fit ----
individual_data <- IndividualData(input_data,
                                  id="claim_number",
                                  continuous_features='AP_i',
                                  categorical_features="claim_type",
                                  accident_period="AM",
                                  calendar_period="RM",
                                  input_time_unit=1/12,
                                  output_time_unit=1/4,
                                  years=4,
                                  continuous_features_spline=TRUE,
                                  degrees_of_freedom=4)

```


```{r}
# cox
individual_data2 <- IndividualData(input_data,
                                  id="claim_number",
                                  categorical_features="claim_type",
                                  continuous_features = NULL,
                                  accident_period="AM",
                                  calendar_period="RM",
                                  input_time_unit=1/12,
                                  output_time_unit=1/4,
                                  years=4,
                                  continuous_features_spline=TRUE,
                                  degrees_of_freedom=4)

  formula_ct <- as.formula(individual_data2$string_formula_i)

  X_i <- pkg.env$model.matrix.creator(data= individual_data2$training.data,
                                        select_columns = c('AP_i',individual_data2$categorical_features))

  hz_names_i = pkg.env$model.matrix.extract.hazard.names(X=X_i,
                                                         string_formula=individual_data2$string_formula_i,
                                                         data=individual_data2$training.data)

  ##################################################################################
  # By now I created a separate function that runs the code. We need to think if there is a smart
  # way of creating only one matrix. We do the same procudure twice.
  # Create X matrix

  X <- pkg.env$model.matrix.creator(data= individual_data2$training.data,
                            select_columns = individual_data2$categorical_features)

  scaler <- pkg.env$scaler(continuous_features_scaling_method=continuous_features_scaling_method)
  Xc <- individual_data2$training.data %>%
    summarize(across(all_of(individual_data2$continuous_features),
                     scaler))



model.out <- pkg.env$fit_cox_model(data=individual_data2$training,
                                                     formula_ct,
                                                     X,
                                                     X_i)

##################################################################################
    # The following steps are data specific.
    # They need to be generalized.

    baseline_out <- pkg.env$hazard_baseline_model(data=individual_data2$training.data,
                                                  cox=model.out$cox,
                                                  hazard=NULL,
                                                  baseline=baseline,
                                                  conversion_factor=individual_data2$conversion_factor,
                                                  nk=50,
                                                  nbin=48,
                                                  phi=1)

    ##################################################################################

```

```{r}
#Arguments 

# hazard_model="deep_surv"
# tie='efron'
# baseline="spline"
# continuous_features_scaling_method="minmax"
# random_seed=1
# hparameters=list(num_nodes=c(10,10),
#                  activation = "LeakyReLU",
#                  batch_size=as.integer(1000),
#                  epochs=as.integer(100),
#                  xi=1,
#                  epsilon = 0,
#                  tie="Efron",
#                  patience = 20,
#                  lr = 0.005,
#                  early_stopping=FALSE)


    


```

```{r}
# Routine
# library(fastDummies)
# formula_ct <- as.formula(individual_data$string_formula_i)
# 
# X_i <- pkg.env$model.matrix.creator(data= individual_data$training.data,
#                                       select_columns = c('AP_i',individual_data$categorical_features))
# 
# hz_names_i = pkg.env$model.matrix.extract.hazard.names(X=X_i,
#                                                        string_formula=individual_data$string_formula_i,
#                                                        data=individual_data$training.data)

##################################################################################
# By now I created a separate function that runs the code. We need to think if there is a smart
# way of creating only one matrix. We do the same procudure twice.
# Create X matrix
# 
# X <- pkg.env$model.matrix.creator(data= individual_data$training.data,
#                           select_columns = individual_data$categorical_features)
# 
# scaler <- pkg.env$scaler(continuous_features_scaling_method=continuous_features_scaling_method)
# Xc <- individual_data$training.data %>%
#   summarize(across(all_of(individual_data$continuous_features),
#                    scaler))

```


```{r}
# 
# 
# training_test_split = pkg.env$check.traintestsplit(percentage_data_training)
# 
# datads_pp = pkg.env$deep_surv_pp(X=cbind(X,Xc),
#                            Y=individual_data$training.data[,c("DP_rev_i", "I", "TR_i")],
#                            training_test_split = training_test_split)


# #Import python modules
# torchtuples <- reticulate::import("torchtuples")
# torch <- reticulate::import("torch")

#Source python code for left truncated deepsurv
# reticulate::source_python("C:\\Users\\gpitt\\Documents\\GitHub\\ReSurv\\inst\\python/coxnetwork_custom.py")
# 
# network_structure=NULL
# 
# if(!("torch.nn.modules.container.Sequential" %in% class(network_structure$net))){
#     net <- torch$nn$Sequential()
#     input_shape =  datads_pp$x_train$shape[[1]]
#     for( i in 1:length(hparameters$num_nodes)+1){
#       if( i > length(hparameters$num_nodes)){
#         # net$append(torch$nn$Linear(input_shape, as.integer(1)))
#         net$add_module('2',torch$nn$Linear(input_shape, as.integer(1)))
#       }
#       else{
#         net$add_module('0',torch$nn$Linear(input_shape,as.integer(hparameters$num_nodes[i])))
#         net$add_module('1',torch$nn[[hparameters$activation]]())
#         input_shape = as.integer(hparameters$num_nodes[i])
#       }
#     }
#   }
# 
# 
#   #Setup batchsize, epochs and verbose settings
#   # batch_size = as.integer(hparameters$batch_size)
#   # 
#   # epochs = as.integer(hparameters$epochs)
# 
#   verbose = T
# 
# 
# 
#   # Setup CoxPH model, as imported from python script.
#   model <- CoxPH(
#     net = net,
#     optimizer = torchtuples$optim$Adam(lr=hparameters$lr),
#     xi=hparameters$xi,
#     eps=hparameters$epsilon,
#     tie = hparameters$tie
#   )
# 
# 
#   #If early stopping specified add to callbacks.
#   if(hparameters$early_stopping==TRUE){
#     callbacks = list(torchtuples$callbacks$EarlyStopping(patience=hparameters$patience))
#   }else{
#     callbacks = NULL
#   }
# 
#   #fit model
#   model$fit(
#     input = datads_pp$x_train,
#     target = datads_pp$y_train,
#     batch_size = hparameters$batch_size,
#     epochs = hparameters$epochs,
#     callbacks = r_to_py(callbacks),
#     verbose = verbose,
#     val_data=datads_pp$validation_data,
#     val_batch_size=hparameters$batch_size
#   )
# 
#   expg <- exp(model$predict(input=datads_pp$x_train, batch_size=hparameters$batch_size))
#   
  
```

```{r}
# prova= model$compute_baseline_hazards(
#       input = datads_pp$x_train,
#       target = datads_pp$y_train,
#       batch_size = hparameters$batch_size)
```









```{r}
# resurv.fit <- ReSurv(individual_data,
#               hazard_model="cox",
#               hparameters=list(early_stopping = FALSE,
#                              patience = 20,
#                               network_structure = NULL,
#                               num_nodes = c(10,10),
#                               activation = "LeakyReLU",
#                               lr = 0.005,
#                               xi=1,
#                               epsilon = 0,
#                               tie="Efron",
#                               batch_size=as.integer(1000),
#                               epochs=as.integer(100)
#                               ))
```

```{r}
# out <- predict(resurv.fit,
#                individual_data)
#
```

