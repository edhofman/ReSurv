---
title: "Comparison with Wuetrich (2018)"
author: "Gabriele Pittarello"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Comparison with Wuetrich (2018)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: '`r system.file("references.bib", package="ReSurv")`'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


library(ReSurv)
# library(ggplot2)
library(data.table)
library(fastDummies)

```


In this vignette, we compare our approach with Wuethrich (2018). We first define some functions that we use to implement the approach from Wuethrich (2018).



```{r}
# Preprocess covariates
## utils
MinMaxScaler <- function(x, na.rm = TRUE) {
  "MinMax Scaler"
  return(2*(x- min(x)) /(max(x)-min(x))-1)
}
## Aggregate in triangles

data_wrapper <- function(data,
                         categorical_features=NULL,
                         continuous_features=NULL,
                         development_period,
                         accident_period){
  
  out <- data[,.(dt_count = .N), by = c(categorical_features,
                         continuous_features,
                         development_period,
                         accident_period)]
  
  return(out)
  
  
}


## actual preprocessing
mw18_datapp_x <- function(data,
                          continuous_features=NULL,
                          categorical_features=NULL) {
  
  # browser()
  
  out <- data 
  
  if (!is.null(categorical_features)) {
    # out_cat=apply(eval(parse(text=paste0("data[,.(",categorical_features,")]"))),2,as.factor)
    
    out[, (categorical_features) := lapply(.SD, as.factor), .SDcols = categorical_features]
    
    # out_cat=apply(eval(parse(text=paste0("data[,.(",categorical_features,")]"))),2,as.factor)
    
  }
  
  if (!is.null(continuous_features)) {
    # out_cont=apply(eval(parse(text=paste0("data[,.(",continuous_features,")]"))),2,MinMaxScaler)
    
    out[, (continuous_features) := lapply(.SD, MinMaxScaler), .SDcols = continuous_features]
  }
  
  to_keep <- c(categorical_features,
               continuous_features)
  
  return(out[,..to_keep])
  
}

# Preprocess targets
mw18_datapp_y <- function(data,
                          development_time,
                          dt_counts,
                          max_dp = 40){
  
 out<-  pivot_wider(data,
                names_from = development_time,
                values_from = dt_counts,
                names_prefix = "DP") 
 
 setDT(out)
 
 out[is.na(out)] <- 0
 
 dp_columns = paste0("DP",1:max_dp)
 
 return(out)
  
}

```

```{r}
#first check: is data sparse enough?

fit_proportional_model <- function(data,
                                max_dp){
  
  
  out <- list()
  
  counts_at_diagonal <- list()
  
  predictions_at_runoff <- list()
  
  for (i in 2:max_dp) {

    proportions_ap <- rep(NA,(max_dp-1))
    
    zero_at_diagonal <- data[[paste0("DP", (max_dp - i + 1))]] == 0 # take them all here and just filer afterwards.
    
    at_diagonal <- data[["AP"]] ==(max_dp - (max_dp - i))
    
    counts_unfiltered <- sum(data[AP < (max_dp - (max_dp - i)), ][[paste0("DP", (max_dp -
                                                                                           i + 1))]])
    
    counts_at_diagonal[[paste0("AP",i)]] <- sum(data[AP == (max_dp - (max_dp - i)), ][[paste0("DP", (max_dp -
                                                                                           i + 1))]])
    
    predicted_cumulatives <- NA
    
    if (sum(zero_at_diagonal & (!at_diagonal)) > 0) {
      
      j = (max_dp - i + 1)
      
      tmp <- data[zero_at_diagonal& (AP < (max_dp - j + 1)),][[paste0("DP", j+1)]]
      
      proportions_ap[j+1] <- sum(tmp)/counts_unfiltered
      
      
      if(i>2){
      
      for (j in (max_dp - i + 1 +1):(max_dp-1)) {
        
        tmp <- data[zero_at_diagonal& (AP < (max_dp - j + 1)),][[paste0("DP", j+1)]]
        tmp_1 <- data[zero_at_diagonal& (AP < (max_dp - j + 1)),][[paste0("DP", j)]]
        
        proportions_ap[j+1] <- sum(tmp)/sum(tmp_1)
        
        
      }}
    }
    
    
    out[[paste0("AP",i)]] <- proportions_ap
    
    predictions_at_runoff[[paste0("AP",i)]]<- counts_at_diagonal[[paste0("AP",i)]]*last(cumprod(proportions_ap[!is.na(proportions_ap)]))
    
  }
  
  total_out <- list(proportions=out,
                    counts_at_diagonal=counts_at_diagonal,
                    predictions_at_runoff=predictions_at_runoff)
  
  return(total_out)
}

```


```{r}

nn_model_fit_and_predict <- function(data,
                                     max_dp,
                                     number_of_neurons,
                                     categorical_features,
                                     continuous_features) {
  
  y_data_columns <- paste0("DP", 1:max_dp)
  x_cat <- dummy_cols(data[,..categorical_features],
                      select_columns=categorical_features,
                      remove_first_dummy=F)[,-1]
  
  
  x_data <- cbind(x_cat,
                  data[,..continuous_features])
  
  y_data <- data[,..y_data_columns]
  
  output_frame <- data.table::copy(data)
  output_frame_score_diagonal <- data.table::copy(data)
  
  for(j in 1:(max_dp-1)){
    
    train_set <- (data[['AP']] < (max_dp - j + 1))
    
    tmp <- y_data[train_set,][[paste0("DP", j+1)]]
    
    tmp_1 <- y_data[train_set,][[paste0("DP", j)]]
    
    cond_positive_denominator <- tmp_1 >0
    
    ytgt = tmp / sqrt(tmp_1)
    wtrain = sqrt(tmp_1)
    
    features <- layer_input(shape=dim(x_data)[2],
                            name = "main_model_il")
    net <- features
    
    if (number_of_neurons > 0) {
      net <- net %>%
        layer_dense(
          units = number_of_neurons,
          activation = 'tanh',
          use_bias = FALSE,
          name = "main_model_hl"
        )
    } 
      net <- net %>%
        layer_dense (units = 1,
                     activation = k_exp,
                     name = "main_model_ol")
    
    
    volumes <- layer_input(shape =c(1),
                           name = "weights_il")
    
    offset <- volumes %>%
      layer_dense( units = 1, activation = 'linear', 
                    use_bias =FALSE , 
                    trainable =FALSE ,
                    weights = list(array(1, dim =c(1 ,1))),
                   name = "weights_ol")
     
    merged <- list(net,
                   offset ) %>%
       layer_multiply()
     model <- keras_model(inputs = list (features , 
                                         volumes), outputs = merged)
     model %>% compile (loss = 'mse', 
                        optimizer = 'rmsprop')
     
     dat.X <- as.matrix.data.frame(x_data[train_set,][cond_positive_denominator,])
     dat.W <- as.matrix(wtrain[cond_positive_denominator])
     dat.Y <- as.matrix(ytgt[cond_positive_denominator])
     
     fit <- model %>% fit(
       list(apply(dat.X,MARGIN = 2,as.numeric), dat.W),
       dat.Y,
       epochs = 500 ,
       verbose = 0,
       batch_size = eval(parse(text=(paste0(as.integer(max(floor(length(ytgt)/10),1)),"L")))),
       validation_split = 0.1
     )
     

     tmp_3 <- y_data[!train_set,][[paste0("DP", j)]]
     
     cond_positive_denominator_test <- tmp_3 >0
     
     x_test <- as.matrix.data.frame(apply(x_data[!train_set,],2,as.numeric))
     
     w_test = as.matrix(sqrt(tmp_3))

     pred <- as.vector(model %>% predict(list(x_test, 
                                              w_test)))*w_test
     
     tmp_4 <- output_frame[!train_set,][[paste0("DP", j)]]
     w_test_1 = as.matrix(sqrt(tmp_4))
     
     pred1 <- as.vector(model %>% predict(list(x_test, 
                                              w_test_1)))*w_test_1
     
  
     pred1[is.na(pred1)] <- tmp_3[is.na(pred1)] #if the nn fits something negative you just put df to one
    
     
    eval(parse(text=paste0("output_frame[!train_set,DP",j+1,":=pred1]")))
    eval(parse(text=paste0("output_frame_score_diagonal[!train_set,DP",j+1,":=pred]")))

    
  }
  
  out <- list(score_total=(output_frame),
              score_diagonal=(output_frame_score_diagonal))
  
  return(out)
  
}


```

```{r}
library(reticulate)
# use_python("C:/Users/pwt887/AppData/Local/Programs/Python/Python313/python.exe")
reticulate::use_condaenv("mw_et")
library(keras)
```




# Scenario Alpha


```{r}

input_data <- data_generator(
  random_seed = 1,
  scenario = 1,
  time_unit = 1 / 4,
  years = 8,
  period_exposure  = 2000
)

```



```{r}
categorical_features = "claim_type"
continuous_features=NULL
development_period = "DP"
accident_period = "AP"
id_col = "claim_number"
max_dp=max(input_data$AP)

```

```{r}
setDT(input_data)
input_data[,DP:=pmin(RP-AP+1,max_dp)]

vec1 <- sort(unique(input_data$DP))
vec2 <- 1:max_dp

v3 <- setdiff(union(vec1, vec2), intersect(vec1, vec2))



```


```{r}
wrapped_data <- data_wrapper(data=input_data,
                             categorical_features = categorical_features,
                             development_period = development_period,
                             accident_period=accident_period)

if(length(v3)!=0){
  
  extra_data <- tail(wrapped_data, 1)

# Duplicate the last row length(v3) times
extra_data <- extra_data[rep(1, length(v3))]

# Assign the DP column
extra_data[, DP := v3]

extra_data[, dt_count := 0]
  wrapped_data <- rbind(wrapped_data,
                        extra_data)
  
}


```

```{r}
wrapped_data<-wrapped_data[order(DP),.SD,by=c(categorical_features,
                               continuous_features,
                               accident_period)]



```



```{r}
dt_target <- mw18_datapp_y(wrapped_data,
              development_time = development_period,
              dt_counts = "dt_count"
              )

columns<-  c(paste0("DP",1:max_dp))


tmp = as.data.frame(t(apply(dt_target[,..columns],1,cumsum)))

setDT(tmp)

dt_target[,(columns):=tmp]




```



```{r}
dt_covariates <- mw18_datapp_x(dt_target,
              categorical_features = categorical_features)


common_cols <- intersect(colnames(dt_covariates),
                         colnames(dt_target))


setDT(dt_covariates)

dt_target[, (common_cols):=dt_covariates[,..common_cols]]

head(dt_target)


```


## Proportional model 

```{r}
out_pm <- fit_proportional_model(data=dt_target,
                                 max_dp = max_dp)


```



## Neural newtorks model 

```{r}
library(keras)
out_nn <- nn_model_fit_and_predict(dt_target,
                                     max_dp=max_dp,
                                     number_of_neurons=1,
                                     categorical_features=categorical_features,
                                     continuous_features=continuous_features)
```


```{r}

scoring_dt <- pivot_longer(
  out_nn$score_total,
  cols = starts_with("DP"),
  names_to="DP",
  values_to = "expected_counts",
  names_prefix = "DP"
) 

setDT(scoring_dt)

# scoring_dt[,DP:=as.numeric(DP)]

true_dt <- pivot_longer(
  dt_target,
  cols = starts_with("DP"),
  names_to="DP",
  values_to = "actual_counts",
  names_prefix = "DP"
) 

setDT(true_dt)

scoring_dt <- merge(scoring_dt,true_dt,by=c("AP","DP",
                                              categorical_features,
                                              continuous_features))

data.table::setkey(scoring_dt,NULL)
# scoring_dt[,ave:=(expected_counts - actual_counts)]

scoring_total<-scoring_dt %>%
  group_by(AP, DP) %>%
  mutate(diff_value = value - lag(value)) %>%
  reframe(abs_ave = abs(sum(ave)),
          I=sum(actual_counts))

are_tot <- sum(scoring_total$abs_ave) / sum(scoring_total$I)


#Cashflow on output scale.Etc quarterly cashflow development
score_diagonal <- resurv_fit_cox$IndividualDataPP$full.data  %>%
  mutate(
    DP_rev_o = floor(max_dp_i * conversion_factor) -
      ceiling(
              DP_i * conversion_factor +
                ((AP_i - 1) %% (
                  1 / conversion_factor
                )) * conversion_factor) + 1,
    AP_o = ceiling(AP_i * conversion_factor)
  ) %>%
  group_by(claim_type, AP_o, DP_rev_o) %>%
  mutate(claim_type = as.character(claim_type)) %>%
  summarize(I = sum(I), .groups = "drop") %>%
  group_by(claim_type, AP_o) %>%
  arrange(desc(DP_rev_o)) %>%
  mutate(I_cum = cumsum(I)) %>%
  mutate(I_cum_lag = lag(I_cum, default = 0)) %>%
  left_join(dfs_output, by = c("AP_o", "claim_type", "DP_rev_o")) %>%
  mutate(I_cum_hat =  I_cum_lag * f_o,
         RP_o = max(DP_rev_o) - DP_rev_o + AP_o) %>%
  inner_join(true_output[, c("AP_o", "DP_rev_o")] %>%  distinct()
             , by = c("AP_o", "DP_rev_o")) %>%
  group_by(AP_o, DP_rev_o) %>%
  reframe(abs_ave2_diag = abs(sum(I_cum_hat) - sum(I_cum)), I = sum(I))

are_cal_q <- sum(score_diagonal$abs_ave2_diag) / sum(score_diagonal$I)






```


```{r}
(sum(out_nn$score_diagonal$DP96)+sum(unlist(out_pm$predictions_at_runoff)))/sum(dt_target$DP96)
# 1.154278
```

## resurv 

```{r}
individual_data <- IndividualDataPP(
  input_data,
  categorical_features = "claim_type",
  continuous_features = "AP",
  accident_period = "AP",
  calendar_period = "RP",
  input_time_granularity = "months",
  output_time_granularity = "years",
  years = 8
)
```


```{r}

resurv_fit_cox <- ReSurv(individual_data,
                     hazard_model = "COX")
```

```{r}
resurv_fit_predict_y <- predict(resurv_fit_cox)
```

```{r}
summary(resurv_fit_predict_y)
```
```{r}
true_diagonal <- sum(unlist(out_pm$counts_at_diagonal))+sum(wrapped_data[AP==1,dt_count])

true_ibnr <- sum(dt_target$DP96)-true_diagonal

mw_ibnr <- (sum(out_nn$DP96)+sum(unlist(out_pm$predictions_at_runoff)))-true_diagonal

resurv_fit_predict_y$predicted_counts


```



# mw 18


```{r}








```



# Scenario 6

```{r}
input_data <- data_generator(
  ref_claim = 200000,
  time_unit = 1 / 4,
  years = 10,
  random_seed=1,
  period_exposure  = 120000,
  period_frequency  = .08,
  scenario=6
)
```

```{r}
categorical_features = "business_use"
continuous_features=NULL
development_period = "DP"
accident_period = "AP"
id_col = "claim_number"
max_dp=max(input_data$AP)
```



```{r}
setDT(input_data)
input_data[,DP:=pmin(RP-AP+1,max_dp)]

vec1 <- sort(unique(input_data$DP))
vec2 <- 1:max_dp

v3 <- setdiff(union(vec1, vec2), intersect(vec1, vec2))



```


```{r}
wrapped_data <- data_wrapper(data=input_data,
                             categorical_features = categorical_features,
                             development_period = development_period,
                             accident_period=accident_period)

if(length(v3)!=0){
  
  extra_data <- tail(wrapped_data, 1)

# Duplicate the last row length(v3) times
extra_data <- extra_data[rep(1, length(v3))]

# Assign the DP column
extra_data[, DP := v3]

extra_data[, dt_count := 0]
  wrapped_data <- rbind(wrapped_data,
                        extra_data)
  
}


```

```{r}
wrapped_data<-wrapped_data[order(DP),.SD,by=c(categorical_features,
                               continuous_features,
                               accident_period)]



```



```{r}
dt_target <- mw18_datapp_y(wrapped_data,
              development_time = development_period,
              dt_counts = "dt_count"
              )

columns<-  c(paste0("DP",1:max_dp))


tmp = as.data.frame(t(apply(dt_target[,..columns],1,cumsum)))

setDT(tmp)

dt_target[,(columns):=tmp]




```



```{r}
dt_covariates <- mw18_datapp_x(dt_target,
              categorical_features = categorical_features)


common_cols <- intersect(colnames(dt_covariates),
                         colnames(dt_target))


setDT(dt_covariates)

dt_target[, (common_cols):=dt_covariates[,..common_cols]]

head(dt_target)


```

```{r}
out_pm <- fit_proportional_model(data=dt_target,
                                 max_dp = max_dp)


```

```{r}

out_nn <- nn_model_fit_and_predict(dt_target,
                                     max_dp=max_dp,
                                     number_of_neurons=1,
                                     categorical_features=categorical_features,
                                     continuous_features=continuous_features)
```

```{r}

```




# Scenario 5

```{r}

input_data <- data_generator(
  ref_claim = 200000,
  time_unit = 1 / 4,
  years = 12,
  random_seed=1,
  period_exposure  = 120000,
  period_frequency  = .08,
  scenario=5
)


hist(input_data$RP-input_data$AP)
```


```{r}
categorical_features = "business_use"
continuous_features=c("property_value","age")
development_period = "DP"
accident_period = "AP"
id_col = "claim_number"
max_dp=max(input_data$AP)
```



```{r}
setDT(input_data)
input_data[,DP:=pmin(RP-AP+1,max_dp)]

vec1 <- sort(unique(input_data$DP))
vec2 <- 1:max_dp

v3 <- setdiff(union(vec1, vec2), intersect(vec1, vec2))



```


```{r}
wrapped_data <- data_wrapper(data=input_data,
                             categorical_features = categorical_features,
                             continuous_features = continuous_features,
                             development_period = development_period,
                             accident_period=accident_period)

if(length(v3)!=0){
  
  extra_data <- tail(wrapped_data, 1)

# Duplicate the last row length(v3) times
extra_data <- extra_data[rep(1, length(v3))]

# Assign the DP column
extra_data[, DP := v3]

extra_data[, dt_count := 0]
  wrapped_data <- rbind(wrapped_data,
                        extra_data)
  
}


```

```{r}
wrapped_data<-wrapped_data[order(DP),.SD,by=c(categorical_features,
                               continuous_features,
                               accident_period)]



```



```{r}
dt_target <- mw18_datapp_y(wrapped_data,
              development_time = development_period,
              dt_counts = "dt_count"
              )

columns<-  c(paste0("DP",1:max_dp))


tmp = as.data.frame(t(apply(dt_target[,..columns],1,cumsum)))

setDT(tmp)

dt_target[,(columns):=tmp]




```

```{r}
dt_covariates <- mw18_datapp_x(dt_target,
              categorical_features = categorical_features,
              continuous_features = continuous_features)


common_cols <- intersect(colnames(dt_covariates),
                         colnames(dt_target))


setDT(dt_covariates)

dt_target[, (common_cols):=dt_covariates[,..common_cols]]




```

```{r}
out_pm <- fit_proportional_model(data=dt_target,
                                 max_dp = max_dp)


```

```{r}

out_nn <- nn_model_fit_and_predict(dt_target,
                                     max_dp=max_dp,
                                     number_of_neurons=2,
                                     categorical_features=categorical_features,
                                     continuous_features=continuous_features)
```




```{r}
sum(input_data$RP>max(input_data$AP))/dim(input_data)[1]
```

```{r}
# hparameters_xgb <- list(
#   params = list(
#     booster = "gbtree",
#     eta = 0.9611239,
#     subsample = 0.62851,
#     alpha = 5.836211,
#     lambda = 15,
#     min_child_weight = 29.18158,
#     max_depth = 1
#   ),
#   print_every_n = 0,
#   nrounds = 3000,
#   verbose = FALSE,
#   early_stopping_rounds = 500
# )

individual_data <- IndividualDataPP(data = input_data,
                                    categorical_features = "business_use",
                                    id="claim_number",
                                    continuous_features = c("AP"),
                                    accident_period = "AP",
                                    calendar_period = "RP",
                                    input_time_granularity = "quarters",
                                    output_time_granularity = "years",
                                    years = 10)

# resurv_fit_xgb <- ReSurv(individual_data,
#                          hazard_model = "XGB",
#                          hparameters = hparameters_xgb)


```


```{r eval=FALSE, include=TRUE}

resurv_fit_cox <- ReSurv(individual_data,
                     hazard_model = "COX")

```

```{r}
hist(resurv_fit_cox$hazard_frame$cum_f_i)
```


```{r}
resurv_fit_predict <- predict(resurv_fit_cox,
                              lower_triangular_output = FALSE)
```


```{r}
hist(resurv_fit_predict$long_triangle_format_out$input_granularity$f_i)
```


```{r}
summary(resurv_fit_predict)
```


```{r}

setDT(input_data)

cl_data <- input_data[,.N,by=.(AP,RP)][RP<=40,DP:=RP-AP+1]

library(ChainLadder)

cl_data <- ChainLadder::as.triangle(cl_data,
                                    value = "N",
                                    origin = "AP",
                                    dev="DP")
cl_data[is.na(cl_data)] <- 0

library(clmplus)
datapp = AggregateDataPP(cumulative.payments.triangle = incr2cum(cl_data), eta= 1/2)

a.model.fit=clmplus(datapp,
                 hazard.model = "a")

a.model=predict(a.model.fit)

sum(a.model$reserve)

```



```{r}
input_data2 <- data_generator(
  ref_claim = 200000,
  time_unit = 1 / 360,
  years = 10,
  random_seed=1,
  period_exposure  = 120,
  period_frequency  = .01,
  scenario=5
)

individual_data2 <- IndividualDataPP(data = input_data2,
                                    categorical_features = "business_use",
                                    continuous_features = c("age"),
                                    accident_period = "AP",
                                    calendar_period = "RP",
                                    input_time_granularity = "days",
                                    output_time_granularity = "years",
                                    years = 10)

resurv_fit_predict <- predict(resurv_fit_cox,newdata = individual_data2)

```

```{r}
summary(resurv_fit_predict)
```



